}
## 2. cleaning the transcripts
# building a function to clean the data
cleanText <- function(x){
# remove new lines CHANGED TO SPACE BECAUSE OF EP 1 FORMATTING
x <- gsub("\n", " ", x)
# remove back slashes
x <- gsub('\"', "", x, fixed = TRUE)
x <- str_trim(x)
x <- str_to_lower(x)
return(x)
}
# buiding dataset id
episodes_id <- str_extract(url, '\\d+-?\\d+')
episodes_trans_clean <- episodes_trans %>%
map(~ cleanText(.x))
episodes_trans_clean <- unlist(episodes_trans_clean, use.names=FALSE)
df_friends <- data.frame(episodes_id,episodes_trans_clean)
View(df_friends)
df_friends <- df_friends %>%
stri_split_lines(episodes_trans_clean)
library(tokenizers)
df_friends <- df_friends %>%
tokenize_sentences(episodes_trans_clean)
tokenize_sentences('I am happy. Yes. I am.')
df_friends <- df_friends %>%
tokenize_sentences(episodes_trans_clean)
episodes_trans_clean <- tokenize_sentences(episodes_trans_clean)
View(episodes_trans_clean)
head(episodes_trans_clean)
episodes_trans_clean <- str_split(episodes_trans_clean, '\\n\\n')
library(htm2txt)
library(dplyr)
library(xml2)
library(httr)
library(stringr)
library(tidyverse)
library(tidytext)
library(rvest)
library(tokenizers)
## 1. scrap from web the friends transcripts
## link: https://fangj.github.io/friends/
# get the urls from the web
link <- "https://fangj.github.io/friends/"
main.page <- read_html(x = link)
url <- main.page %>% # feed `main.page` to the next step
html_nodes("a") %>% # get the CSS nodes
html_attr("href") # extract the URLs
urls <- paste(link,url,sep = '') # obtain the full urls to get the text from website
episodes_trans <- vector() #build an empty vector for adding the data later
# adding the text into the vector
for (u in urls){
episodes_trans <- c(episodes_trans,gettxt(u))
}
## 2. cleaning the transcripts
# building a function to clean the data
cleanText <- function(x){
# remove new lines CHANGED TO SPACE BECAUSE OF EP 1 FORMATTING
x <- gsub("\n", " ", x)
# remove back slashes
x <- gsub('\"', "", x, fixed = TRUE)
x <- str_trim(x)
x <- str_to_lower(x)
return(x)
}
# buiding dataset id
episodes_id <- str_extract(url, '\\d+-?\\d+')
episodes_trans_clean <- episodes_trans %>%
map(~ cleanText(.x))
episodes_trans_clean <- unlist(episodes_trans_clean, use.names=FALSE)
episodes_trans_clean <- str_split(episodes_trans_clean, '\\n\\n')
View(episodes_trans_clean)
head(episodes_trans_clean)
episodes_trans_clean <- str_split(episodes_trans_clean, '  ')
View(episodes_trans_clean)
head(episodes_trans_clean)
df_friends <- tibble(episodes_id,episodes_trans_clean)%>%
unnest(clean_text) %>%
filter(str_detect(clean_text, "")) %>%
# determining whether a line is a scene, title, action or person speaking
mutate(type = ifelse(str_detect(episodes_trans_clean, "^(\\[sc)|(\\(at)|(\\[at)"), "scene", # scenes are on singular lines, enclosed in square brackets annd ending in round
ifelse(str_detect(episodes_trans_clean, "^(\\()|(\\<)"), "action",  # actions are on singular lines, enclosed in round and angularbrackets
ifelse(str_detect(episodes_trans_clean, "^written"), "written", # indicates who the episode was written by
ifelse(str_detect(episodes_trans_clean, "^[a-z](.*):"), "person",  # if doesn't match anything above, and has a semicolon, should be a person speaking
NA)))))
df_friends <- tibble(episodes_id,episodes_trans_clean)%>%
unnest(episodes_trans_clean) %>%
filter(episodes_trans_clean, "") %>%
# determining whether a line is a scene, title, action or person speaking
mutate(type = ifelse(str_detect(episodes_trans_clean, "^(\\[sc)|(\\(at)|(\\[at)"), "scene", # scenes are on singular lines, enclosed in square brackets annd ending in round
ifelse(str_detect(episodes_trans_clean, "^(\\()|(\\<)"), "action",  # actions are on singular lines, enclosed in round and angularbrackets
ifelse(str_detect(episodes_trans_clean, "^written"), "written", # indicates who the episode was written by
ifelse(str_detect(episodes_trans_clean, "^[a-z](.*):"), "person",  # if doesn't match anything above, and has a semicolon, should be a person speaking
NA)))))
View(df_friends)
df_friends <- tibble(episodes_id,episodes_trans_clean)
View(df_friends)
head(df_friends,1)
df_friends <- tibble(episodes_id,episodes_trans_clean)%>%
unnest(episodes_trans_clean)
head(df_friends,1)
df_friends <- tibble(episodes_id,episodes_trans_clean)%>%
unnest(episodes_trans_clean) %>%
filter(episodes_trans_clean, "") %>%
# determining whether a line is a scene, title, action or person speaking
mutate(type = ifelse(str_detect(episodes_trans_clean, "^(\\[sc)|(\\(at)|(\\[at)"), "scene", # scenes are on singular lines, enclosed in square brackets annd ending in round
ifelse(str_detect(episodes_trans_clean, "^(\\()|(\\<)"), "action",  # actions are on singular lines, enclosed in round and angularbrackets
ifelse(str_detect(episodes_trans_clean, "^written"), "written", # indicates who the episode was written by
ifelse(str_detect(episodes_trans_clean, "^[a-z](.*):"), "person",  # if doesn't match anything above, and has a semicolon, should be a person speaking
NA)))))
library(htm2txt)
library(dplyr)
library(xml2)
library(httr)
library(stringr)
library(tidyverse)
library(tidytext)
library(rvest)
library(tokenizers)
## 1. scrap from web the friends transcripts
## link: https://fangj.github.io/friends/
# get the urls from the web
link <- "https://fangj.github.io/friends/"
main.page <- read_html(x = link)
url <- main.page %>% # feed `main.page` to the next step
html_nodes("a") %>% # get the CSS nodes
html_attr("href") # extract the URLs
urls <- paste(link,url,sep = '') # obtain the full urls to get the text from website
episodes_trans <- vector() #build an empty vector for adding the data later
# adding the text into the vector
for (u in urls){
episodes_trans <- c(episodes_trans,gettxt(u))
}
## 2. cleaning the transcripts
# building a function to clean the data
cleanText <- function(x){
# remove new lines CHANGED TO SPACE BECAUSE OF EP 1 FORMATTING
x <- gsub("\n", " ", x)
# remove back slashes
x <- gsub('\"', "", x, fixed = TRUE)
x <- str_trim(x)
x <- str_to_lower(x)
return(x)
}
# buiding dataset id
episodes_id <- str_extract(url, '\\d+-?\\d+')
episodes_trans_clean <- episodes_trans %>%
map(~ cleanText(.x))
episodes_trans_clean <- unlist(episodes_trans_clean, use.names=FALSE)
episodes_trans_clean <- str_split(episodes_trans_clean, '  ')
df_friends <- tibble(episodes_id,episodes_trans_clean)%>%
unnest(episodes_trans_clean) %>%
filter(episodes_trans_clean, "") %>%
# determining whether a line is a scene, title, action or person speaking
mutate(type =   ifelse(str_detect(clean_text, "^(\\[sc)|(\\(at)|(\\[at)"), "scene",      # scenes are on singular lines, enclosed in square brackets annd ending in round
ifelse(str_detect(clean_text, "^(\\()|(\\<)"), "action",              # actions are on singular lines, enclosed in round and angularbrackets
ifelse(str_detect(clean_text, "^written"), "written",     # indicates who the episode was written by
ifelse(str_detect(clean_text, "^[a-z](.*):"), "person",  # if doesn't match anything above, and has a semicolon, should be a person speaking
NA)))))
df_friends <- tibble(episodes_id,episodes_trans_clean)%>%
unnest(episodes_trans_clean) %>%
filter(episodes_trans_clean, "") %>%
# determining whether a line is a scene, title, action or person speaking
mutate(type =  ifelse(str_detect(episodes_trans_clean, "^(\\[sc)|(\\(at)|(\\[at)"), "scene",      # scenes are on singular lines, enclosed in square brackets annd ending in round
ifelse(str_detect(episodes_trans_clean, "^(\\()|(\\<)"), "action",              # actions are on singular lines, enclosed in round and angularbrackets
ifelse(str_detect(episodes_trans_clean, "^written"), "written",     # indicates who the episode was written by
ifelse(str_detect(episodes_trans_clean, "^[a-z](.*):"), "person",  # if doesn't match anything above, and has a semicolon, should be a person speaking
NA)))))
df_friends <- tibble(episodes_id,episodes_trans_clean)%>%
unnest(episodes_trans_clean) %>%
filter(episodes_trans_clean, "")
df_friends <- tibble(episodes_id,episodes_trans_clean)%>%
unnest(episodes_trans_clean) %>%
filter(episodes_trans_clean, " ") %>%
# determining whether a line is a scene, title, action or person speaking
mutate(type =  ifelse(str_detect(episodes_trans_clean, "^(\\[sc)|(\\(at)|(\\[at)"), "scene",      # scenes are on singular lines, enclosed in square brackets annd ending in round
ifelse(str_detect(episodes_trans_clean, "^(\\()|(\\<)"), "action",              # actions are on singular lines, enclosed in round and angularbrackets
ifelse(str_detect(episodes_trans_clean, "^written"), "written",     # indicates who the episode was written by
ifelse(str_detect(episodes_trans_clean, "^[a-z](.*):"), "person",  # if doesn't match anything above, and has a semicolon, should be a person speaking
NA)))))
df_friends <- tibble(episodes_id,episodes_trans_clean)%>%
unnest(episodes_trans_clean) %>%
# determining whether a line is a scene, title, action or person speaking
mutate(type =  ifelse(str_detect(episodes_trans_clean, "^(\\[sc)|(\\(at)|(\\[at)"), "scene",      # scenes are on singular lines, enclosed in square brackets annd ending in round
ifelse(str_detect(episodes_trans_clean, "^(\\()|(\\<)"), "action",              # actions are on singular lines, enclosed in round and angularbrackets
ifelse(str_detect(episodes_trans_clean, "^written"), "written",     # indicates who the episode was written by
ifelse(str_detect(episodes_trans_clean, "^[a-z](.*):"), "person",  # if doesn't match anything above, and has a semicolon, should be a person speaking
NA)))))
View(df_friends)
main_char <- c("chandler", "ross", "monica", "phoebe", "joey", "rachel")
name_map <- data.frame(main) %>% t() %>% set_names("chan", "ross", "mnca", "phoe", "joey", "rach")
main_char <- c("chandler", "ross", "monica", "phoebe", "joey", "rachel")
name_map <- data.frame(main_char) %>% t() %>% set_names("chan", "ross", "mnca", "phoe", "joey", "rach")
View(name_map)
clean_df_friends <- df_friends %>%
separate(episodes_trans_clean, into = c("person", "line"), sep = ":") %>%
mutate(line = str_trim(line)) %>%
mutate(person = ifelse(person %in% names(name_map), name_map[person], person)) %>%
mutate(season = substr(episodes_id, 1, 2))
View(clean_df_friends)
View(clean_df_friends)
library(htm2txt)
library(dplyr)
library(xml2)
library(httr)
library(stringr)
library(tidyverse)
library(tidytext)
library(rvest)
library(tokenizers)
## 1. scrap from web the friends transcripts
## link: https://fangj.github.io/friends/
# get the urls from the web
link <- "https://fangj.github.io/friends/"
main.page <- read_html(x = link)
url <- main.page %>% # feed `main.page` to the next step
html_nodes("a") %>% # get the CSS nodes
html_attr("href") # extract the URLs
urls <- paste(link,url,sep = '') # obtain the full urls to get the text from website
episodes_trans <- vector() #build an empty vector for adding the data later
# adding the text into the vector
for (u in urls){
episodes_trans <- c(episodes_trans,gettxt(u))
}
## 2. cleaning the transcripts
# building a function to clean the data
cleanText <- function(x){
# remove new lines CHANGED TO SPACE BECAUSE OF EP 1 FORMATTING
x <- gsub("\n", " ", x)
# remove back slashes
x <- gsub('\"', "", x, fixed = TRUE)
x <- str_trim(x)
x <- str_to_lower(x)
return(x)
}
# buiding dataset id
episodes_id <- str_extract(url, '\\d+-?\\d+')
episodes_trans_clean <- episodes_trans %>%
map(~ cleanText(.x))
episodes_trans_clean <- unlist(episodes_trans_clean, use.names=FALSE)
episodes_trans_clean <- str_split(episodes_trans_clean, '  ')
df_friends <- tibble(episodes_id,episodes_trans_clean)%>%
unnest(episodes_trans_clean) %>%
# determining whether a line is a scene, title, action or person speaking
mutate(type =  ifelse(str_detect(episodes_trans_clean, "^(\\[sc)|(\\(at)|(\\[at)"), "scene",      # scenes are on singular lines, enclosed in square brackets annd ending in round
ifelse(str_detect(episodes_trans_clean, "^(\\()|(\\<)"), "action",              # actions are on singular lines, enclosed in round and angularbrackets
ifelse(str_detect(episodes_trans_clean, "^written"), "written",     # indicates who the episode was written by
ifelse(str_detect(episodes_trans_clean, "^[a-z](.*):"), "person",  # if doesn't match anything above, and has a semicolon, should be a person speaking
NA)))))
clean_df_friends <- df_friends %>%
if(type == 'person'){
separate(episodes_trans_clean, into = c("person", "line"), sep = ":") %>%
mutate(line = str_trim(line))
}%>%
mutate(season = substr(episodes_id, 1, 2))
clean_df_friends <- df_friends %>%
if(clean_df_friends$type == 'person'){
separate(episodes_trans_clean, into = c("person", "line"), sep = ":") %>%
mutate(line = str_trim(line))
}%>%
mutate(season = substr(episodes_id, 1, 2))
df_friends %>%
filter(type == 'person')%>%
separate(episodes_trans_clean, into = c("person", "line"), sep = ":") %>%
mutate(line = str_trim(line))
if (df_friends$type == 'person'){
separate(df_friends$episodes_trans_clean, into = c("person", "line"), sep = ":")
mutate(line = str_trim(line))
}
View(df_friends)
df_friends <- dat %>%
separate(episodes_trans_clean, into = c("person", "line"), sep = ":") %>%
mutate(line = str_trim(line)) %>%
mutate(season = substr(id, 1, 2))
df_friends <- df_friends %>%
separate(episodes_trans_clean, into = c("person", "line"), sep = ":") %>%
mutate(line = str_trim(line)) %>%
mutate(season = substr(id, 1, 2))
friends_lines <- friends_script %>%
filter(type %in% c("person", "scene")) %>%
group_by(episode_id) %>%
mutate(scene = ifelse(type == "scene", 1, NA)) %>%
mutate(scene2 = cumsum(0 + !is.na(scene))) %>%
ungroup() %>%
select(-scene)
friends_lines <- df_friends %>%
filter(type %in% c("person", "scene")) %>%
group_by(episode_id) %>%
mutate(scene = ifelse(type == "scene", 1, NA)) %>%
mutate(scene2 = cumsum(0 + !is.na(scene))) %>%
ungroup() %>%
select(-scene)
friends_lines <- df_friends %>%
filter(type %in% c("person", "scene")) %>%
group_by(episodes_id) %>%
mutate(scene = ifelse(type == "scene", 1, NA)) %>%
mutate(scene2 = cumsum(0 + !is.na(scene))) %>%
ungroup() %>%
select(-scene)
View(df_friends)
df_friends <- df_friends %>%
separate(episodes_trans_clean, into = c("person", "line"), sep = ":") %>%
mutate(line = str_trim(line)) %>%
mutate(season = substr(id, 1, 2))
df_friends <- df_friends %>%
separate(episodes_trans_clean, into = c("person", "line"), sep = ":") %>%
mutate(line = str_trim(line)) %>%
mutate(season = substr(episodes_id, 1, 2))
friends_lines <- df_friends %>%
filter(type %in% c("person", "scene")) %>%
group_by(episodes_id) %>%
mutate(scene = ifelse(type == "scene", 1, NA)) %>%
mutate(scene2 = cumsum(0 + !is.na(scene))) %>%
ungroup() %>%
select(-scene)
View(friends_lines)
library(htm2txt)
library(dplyr)
library(xml2)
library(httr)
library(stringr)
library(tidyverse)
library(tidytext)
library(rvest)
library(tokenizers)
## 1. scrap from web the friends transcripts
## link: https://fangj.github.io/friends/
# get the urls from the web
link <- "https://fangj.github.io/friends/"
main.page <- read_html(x = link)
url <- main.page %>% # feed `main.page` to the next step
html_nodes("a") %>% # get the CSS nodes
html_attr("href") # extract the URLs
urls <- paste(link,url,sep = '') # obtain the full urls to get the text from website
episodes_trans <- vector() #build an empty vector for adding the data later
# adding the text into the vector
for (u in urls){
episodes_trans <- c(episodes_trans,gettxt(u))
}
## 2. cleaning the transcripts
# building a function to clean the data
cleanText <- function(x){
# remove new lines CHANGED TO SPACE BECAUSE OF EP 1 FORMATTING
x <- gsub("\n", " ", x)
# remove back slashes
x <- gsub('\"', "", x, fixed = TRUE)
x <- str_trim(x)
x <- str_to_lower(x)
return(x)
}
# buiding dataset id
episodes_id <- str_extract(url, '\\d+-?\\d+')
episodes_trans_clean <- episodes_trans %>%
map(~ cleanText(.x))
episodes_trans_clean <- unlist(episodes_trans_clean, use.names=FALSE)
episodes_trans_clean <- str_split(episodes_trans_clean, '  ')
df_friends <- tibble(episodes_id,episodes_trans_clean)%>%
unnest(episodes_trans_clean) %>%
# determining whether a line is a scene, title, action or person speaking
mutate(type =  ifelse(str_detect(episodes_trans_clean, "^(\\[sc)|(\\(at)|(\\[at)"), "scene",      # scenes are on singular lines, enclosed in square brackets annd ending in round
ifelse(str_detect(episodes_trans_clean, "^(\\()|(\\<)"), "action",              # actions are on singular lines, enclosed in round and angularbrackets
ifelse(str_detect(episodes_trans_clean, "^written"), "written",     # indicates who the episode was written by
ifelse(str_detect(episodes_trans_clean, "^[a-z](.*):"), "person",  # if doesn't match anything above, and has a semicolon, should be a person speaking
NA)))))
df_friends <- df_friends %>%
separate(episodes_trans_clean, into = c("person", "line"), sep = ":") %>%
mutate(line = str_trim(line)) %>%
mutate(season = substr(episodes_id, 1, 2))
friends_lines <- df_friends %>%
filter(type %in% c("person", "scene")) %>%
group_by(episodes_id) %>%
mutate(scene = ifelse(type == "scene", 1, NA)) %>%
mutate(scene2 = cumsum(0 + !is.na(scene))) %>%
ungroup() %>%
select(-scene)
usethis::use_data("friends_lines")
usethis::use_data(friends_lines)
write_csv(friends_lines, "/data/friends_lines.csv")
write_csv(friends_lines, "/friends_lines.csv")
usethis::use_data(friends_lines,overwrite = TRUE)
write_csv(friends_lines, "data-raw/friends_lines.csv")
use_readme_rmd()
use_news_md()
use_vignette("FriendsScripts")
library(usethis)
library(devtools)
use_readme_rmd()
use_news_md()
use_vignette("FriendsScripts")
use_github_links()
use_github_links()
library(FriendsScripts)
check()
check()
library(usethis)
library(usethis)
check()
library(devtools)
check()
load("D:/Github/Jiaqing_Ge/final project/Final project 2/FriendsScripts/data/friends_lines.rda")
str(friends_lines)
check()
check()
check()
library(FriendsScripts)
# average number of scenes across all episodes
friends_lines %>%
group_by(id) %>%
summarize(max = max(scene2)) %>% ungroup() %>%
summarize(avg_scenes = mean(max))
library(FriendsScripts)
library(dplyr)
friends_lines %>%
group_by(id) %>%
summarize(max = max(scene2)) %>% ungroup() %>%
summarize(avg_scenes = mean(max))
friends_lines %>%
group_by(episode_id) %>%
summarize(max = max(scene2)) %>% ungroup() %>%
summarize(avg_scenes = mean(max))
# average number of scenes across all episodes
friends_lines %>%
group_by(episodes_id) %>%
summarize(max = max(scene2)) %>% ungroup() %>%
summarize(avg_scenes = mean(max))
friends_lines %>% filter(type %in% c("person", "scene")) %>% count(id) %>% arrange(n)
friends_lines %>% filter(type %in% c("person", "scene")) %>% count(episodes_id) %>% arrange(n)
# total number of scenes
friends_lines %>%
group_by(id) %>%
count(scene2) %>%
summarize(max = max(scene2)) %>%
arrange(max) %>% ungroup() %>%
summarize(num_scenes = sum(max))
# total number of scenes
friends_lines %>%
group_by(episodes_id) %>%
count(scene2) %>%
summarize(max = max(scene2)) %>%
arrange(max) %>% ungroup() %>%
summarize(num_scenes = sum(max))
friends_lines %>%
filter(person %in% main) %>%
count(person, id, sort = TRUE) %>%
mutate(new_id = as.factor(id)) %>%
ggplot(aes(x = new_id, y = n)) +
geom_point(aes(colour = person), size = 0.5, alpha = 0.8) +
geom_smooth(stat = "smooth", se = FALSE, method = "auto", aes(group = person, colour = person), alpha = 0.8) +
scale_fill_brewer(palette = "Dark2") +
theme_dark_ds(base_size = 16) +
ggtitle("Friends - # of Lines by Episode")
main_char <- c("chandler", "ross", "monica", "phoebe", "joey", "rachel")
friends_lines %>%
filter(person %in% main) %>%
count(person, episodes_id, sort = TRUE) %>%
mutate(new_id = as.factor(episodes_id)) %>%
ggplot(aes(x = new_id, y = n)) +
geom_point(aes(colour = person), size = 0.5, alpha = 0.8) +
geom_smooth(stat = "smooth", se = FALSE, method = "auto", aes(group = person, colour = person), alpha = 0.8) +
scale_fill_brewer(palette = "Dark2") +
theme_dark_ds(base_size = 16) +
ggtitle("Friends - # of Lines by Episode")
main_char <- c("chandler", "ross", "monica", "phoebe", "joey", "rachel")
friends_lines %>%
filter(person %in% main_char) %>%
count(person, episodes_id, sort = TRUE) %>%
mutate(new_id = as.factor(episodes_id)) %>%
ggplot(aes(x = new_id, y = n)) +
geom_point(aes(colour = person), size = 0.5, alpha = 0.8) +
geom_smooth(stat = "smooth", se = FALSE, method = "auto", aes(group = person, colour = person), alpha = 0.8) +
scale_fill_brewer(palette = "Dark2") +
theme_dark_ds(base_size = 16) +
ggtitle("Friends - # of Lines by Episode")
library(ggplot2)
main_char <- c("chandler", "ross", "monica", "phoebe", "joey", "rachel")
friends_lines %>%
filter(person %in% main_char) %>%
count(person, episodes_id, sort = TRUE) %>%
mutate(new_id = as.factor(episodes_id)) %>%
ggplot(aes(x = new_id, y = n)) +
geom_point(aes(colour = person), size = 0.5, alpha = 0.8) +
geom_smooth(stat = "smooth", se = FALSE, method = "auto", aes(group = person, colour = person), alpha = 0.8) +
scale_fill_brewer(palette = "Dark2") +
theme_dark_ds(base_size = 16) +
ggtitle("Friends - # of Lines by Episode")
library(ggplot2)
main_char <- c("chandler", "ross", "monica", "phoebe", "joey", "rachel")
friends_lines %>%
filter(person %in% main_char) %>%
count(person, episodes_id, sort = TRUE) %>%
mutate(new_id = as.factor(episodes_id)) %>%
ggplot(aes(x = new_id, y = n)) +
geom_point(aes(colour = person), size = 0.5, alpha = 0.8) +
geom_smooth(stat = "smooth", se = FALSE, method = "auto", aes(group = person, colour = person), alpha = 0.8) +
scale_fill_brewer(palette = "Dark2") +
ggtitle("Friends - # of Lines by Episode")
library(ggplot2)
main_char <- c("chandler", "ross", "monica", "phoebe", "joey", "rachel")
friends_lines %>%
filter(person %in% main_char) %>%
count(person, episodes_id, sort = TRUE) %>%
mutate(new_id = as.factor(episodes_id)) %>%
ggplot(aes(x = new_id, y = n)) +
geom_point(aes(colour = person), size = 0.5, alpha = 0.8) +
geom_smooth(stat = "smooth", se = FALSE, method = "auto", aes(group = person, colour = person), alpha = 0.8) +
scale_fill_brewer(palette = "Dark2") +
ggtitle("Friends - Number of Lines by Episode")
library(usethis)
library(devtools)
check()
use_github_links()
use_github_links()
